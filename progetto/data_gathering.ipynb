{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acda10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0988e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = open(\"../esercizi_classe/api_key.txt\").read()\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c690f5",
   "metadata": {},
   "source": [
    "Ricerca e salvataggio di tutti i video per canale nel periodo temporale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5470149",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\"Romeo Agresti\", \"Il BiancoNero\", \"Colpo Gobbo\",\"Luca Toselli\",\"lAngolodiKinoshi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb443b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDfromName(name):\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        q=name,\n",
    "        type=\"channel\",\n",
    "        maxResults=5\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items'][0]['id']['channelId']\n",
    "\n",
    "def getChannelPlaylist(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "def get_videos_from_channel(playlist_id,channel_name):\n",
    "    begin_date = datetime(2024,7,7)\n",
    "    end_date = datetime(2025,3,24)\n",
    "    video_ids_and_dates = []\n",
    "    next_page_token = None\n",
    "    while True:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            video_date = datetime.strptime(item['snippet']['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            if begin_date <= video_date <= end_date:\n",
    "                video_ids_and_dates.append((item['snippet']['resourceId']['videoId'], video_date))\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    with open(f\"video_ids_{channel_name}.csv\", \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for video_id in video_ids_and_dates:\n",
    "            writer.writerow([video_id[0],video_id[1].strftime(\"%Y-%m-%d\")])\n",
    "    print(f\"Video IDs for {channel_name} saved to video_ids_{channel_name}.csv\")\n",
    "\n",
    "def get_csv_files(channels):\n",
    "    for channel in channels:\n",
    "        channel_id = getIDfromName(channel)\n",
    "        playlist_id = getChannelPlaylist(channel_id)\n",
    "        get_videos_from_channel(playlist_id,channel)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_csv_files(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c4081",
   "metadata": {},
   "source": [
    "<p>Reperimento dei commenti:\n",
    "<p>-per ogni commento salvo id, video commentato, autore, contenuto, data, likes, id commento a cui risponde (se c'Ã¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comment:\n",
    "    def __init__(self, id, video_id, content, author,date, likes, reply_to_id=None):\n",
    "        self.id = id\n",
    "        self.video_id = video_id\n",
    "        self.content = content\n",
    "        self.author = author\n",
    "        self.date = date\n",
    "        self.likes = likes\n",
    "        self.reply_to_id = reply_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_one_vid(video_id):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet,replies\",\n",
    "        videoId=video_id,\n",
    "        textFormat=\"plainText\",\n",
    "        maxResults=100\n",
    "    )\n",
    "    response = request.execute()\n",
    "    comments.extend(get_comments_from_response(response[\"items\"]))\n",
    "    next_page_token = response.get(\"nextPageToken\", None)\n",
    "    while next_page_token:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments.extend(get_comments_from_response(response[\"items\"]))\n",
    "        next_page_token = response.get(\"nextPageToken\", None)\n",
    "    return comments\n",
    "\n",
    "def get_comments_from_response(items):\n",
    "    comments = []\n",
    "    for item in items:\n",
    "        main_comment = item[\"snippet\"][\"topLevelComment\"]\n",
    "        comment_id = main_comment[\"id\"]\n",
    "        video_id = item[\"snippet\"][\"videoId\"]\n",
    "        author = main_comment[\"snippet\"][\"authorDisplayName\"]\n",
    "        content = main_comment[\"snippet\"][\"textDisplay\"]\n",
    "        date = datetime.strptime(main_comment[\"snippet\"][\"publishedAt\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        likes = main_comment[\"snippet\"][\"likeCount\"]\n",
    "        comments.append(Comment(comment_id, video_id, content, author, date, likes))\n",
    "        if \"replies\" in item:\n",
    "            for reply in item[\"replies\"][\"comments\"]:\n",
    "                reply_id = reply[\"id\"]\n",
    "                reply_content = reply[\"snippet\"][\"textDisplay\"]\n",
    "                reply_author = reply[\"snippet\"][\"authorDisplayName\"]\n",
    "                reply_date = datetime.strptime(reply[\"snippet\"][\"publishedAt\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                reply_likes = reply[\"snippet\"][\"likeCount\"]\n",
    "                comments.append(Comment(reply_id, video_id, reply_content, reply_author, reply_date, reply_likes, comment_id))\n",
    "    return comments\n",
    "\n",
    "def save_comments_csv(comments, channel_name):\n",
    "    with open(f\"comments_{channel_name}.csv\", \"w\", newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Comment ID\", \"Video ID\", \"Content\", \"Author\", \"Date\", \"Likes\", \"Reply To ID\"])\n",
    "        for comment in comments:\n",
    "            writer.writerow([comment.id, comment.video_id, comment.content, comment.author, comment.date.strftime(\"%Y-%m-%d\"), comment.likes, comment.reply_to_id])\n",
    "    print(f\"Comments for {channel_name} saved to comments_{channel_name}.csv\")\n",
    "\n",
    "def get_comments_from_csv_file(channel):\n",
    "    with open(f\"video_ids_{channel}.csv\", \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        video_ids = [row[0] for row in reader]\n",
    "    all_comments = []\n",
    "    for video_id in video_ids:\n",
    "        comments = get_comments_one_vid(video_id)\n",
    "        all_comments.extend(comments)\n",
    "    save_comments_csv(all_comments, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1600462",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    get_comments_from_csv_file(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22735064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_set(channel_name):\n",
    "    user_set = set()\n",
    "    with open(f\"comments_{channel_name}.csv\", \"r\", encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            user_set.add(row[3])  # Assuming the author is in the 4th column\n",
    "    with open(f\"user_set_{channel_name}.csv\", \"w\",newline=\"\", encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for user in user_set:\n",
    "            writer.writerow([user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    create_user_set(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_commented_by_user(channel):\n",
    "    user_and_videos = {}\n",
    "    with open(f\"comments_{channel}.csv\", \"r\", encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            user = row[\"Author\"]\n",
    "            video_id = row[\"Video ID\"]\n",
    "            if user not in user_and_videos:\n",
    "                user_and_videos[user] = set()\n",
    "            if video_id not in user_and_videos[user]:\n",
    "                user_and_videos[user].add(video_id)\n",
    "    with open(f\"user_and_videos_{channel}.csv\", \"w\", newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"User\", \"Video ID\"])\n",
    "        for user, videos in user_and_videos.items():\n",
    "            for video in videos:\n",
    "                writer.writerow([user, video])\n",
    "    print(f\"User and videos for {channel} saved to user_and_videos_{channel}.csv\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a45658",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    get_videos_commented_by_user(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a01226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_user_and_videos_csv(channel):\n",
    "    user_and_videos = {}\n",
    "    with open(f\"user_and_videos_{channel}.csv\", \"r\", encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            user = row[\"User\"]\n",
    "            video_id = row[\"Video ID\"]\n",
    "            if user not in user_and_videos:\n",
    "                user_and_videos[user] = set()\n",
    "            user_and_videos[user].add(video_id)\n",
    "    return user_and_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53936f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = nx.Graph()\n",
    "for channel in channels:\n",
    "    user_and_videos = read_user_and_videos_csv(channel)\n",
    "    for user1, user2 in combinations(user_and_videos.keys(), 2):\n",
    "        common_videos = user_and_videos[user1].intersection(user_and_videos[user2])\n",
    "        if len(common_videos) > 1:\n",
    "            G2.add_edge(user1, user2, weight=len(common_videos))\n",
    "nx.write_gexf(G2, \"user_common_videos_graph_2.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo = nx.read_gexf(\"user_common_videos_graph_2.gexf\")\n",
    "eigenvector_centrality = nx.eigenvector_centrality(grafo,200)\n",
    "node_e_centr = list(eigenvector_centrality.items())\n",
    "node_e_centr.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_e_centr[:math.floor(0.1*len(node_e_centr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_and_betweenness = nx.betweenness_centrality(grafo,200)\n",
    "node_bet_centr = list(node_and_betweenness.items())\n",
    "node_bet_centr.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeCentralities:\n",
    "    def __init__(self, node, degree, eigenvector_centrality, betweenness_centrality,degree_centrality):\n",
    "        self.node = node\n",
    "        self.degree = degree\n",
    "        self.eigenvector_centrality = eigenvector_centrality\n",
    "        self.betweenness_centrality = betweenness_centrality\n",
    "        self.degree_centrality = degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_and_degree = nx.degree_centrality(grafo)\n",
    "node_degree_centr = list(node_and_degree.items())\n",
    "node_degree_centr.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo.degree(grafo.nodes(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c12261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_centralities_list(node_and_degree, node_e_centr, node_bet_centr):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all videos and number of comments\n",
    "def get_all_videos_and_comments():\n",
    "    all_videos = {}\n",
    "    for channel in channels:\n",
    "        if channel != \"Romeo Agresti\":\n",
    "            continue\n",
    "        with open(f\"comments_{channel}.csv\", \"r\", encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                video_id = row[\"Video ID\"]\n",
    "                if video_id not in all_videos:\n",
    "                    all_videos[video_id] = 0\n",
    "                all_videos[video_id] += 1\n",
    "    return all_videos\n",
    "all_videos = get_all_videos_and_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_per_week():\n",
    "    videos_per_week = {}\n",
    "    videos_and_comments = {}\n",
    "    for channel in channels:\n",
    "        if channel != \"Romeo Agresti\":\n",
    "            continue\n",
    "        with open(f\"video_ids_{channel}.csv\", \"r\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                video_id = row[0]\n",
    "                date_str = datetime.strptime(row[1], \"%Y-%m-%d\")\n",
    "                week_start = date_str - timedelta(days=date_str.weekday())\n",
    "                if week_start not in videos_per_week:\n",
    "                    videos_per_week[week_start] = []\n",
    "                videos_per_week[week_start].append(video_id)\n",
    "        with open(f\"comments_{channel}.csv\", \"r\", encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                video_id = row[\"Video ID\"]\n",
    "                if video_id not in videos_and_comments:\n",
    "                    videos_and_comments[video_id] = 0\n",
    "                videos_and_comments[video_id] += 1\n",
    "\n",
    "    return videos_per_week, videos_and_comments\n",
    "videos_per_week, videos_and_comments = get_videos_per_week()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc65c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_commented_videos_per_week(n):\n",
    "    videos_per_week, videos_and_comments = get_videos_per_week()\n",
    "    top_commented_videos = {}\n",
    "    for week_start, videos in videos_per_week.items():\n",
    "        # Sort videos by number of comments\n",
    "        for video in videos:\n",
    "            if video in videos_and_comments:\n",
    "                if week_start not in top_commented_videos:\n",
    "                    top_commented_videos[week_start] = []\n",
    "                top_commented_videos[week_start].append((video, videos_and_comments[video]))\n",
    "        top_commented_videos[week_start].sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(top_commented_videos[week_start]) > n:\n",
    "            top_commented_videos[week_start] = top_commented_videos[week_start][:n]\n",
    "    return top_commented_videos\n",
    "top_commented_videos = get_top_commented_videos_per_week(1)\n",
    "top_commented_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b609c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_and_video = {}\n",
    "for channel in channels:\n",
    "    # if channel != \"Romeo Agresti\":\n",
    "    #     continue\n",
    "    with open(f\"comments_{channel}.csv\", \"r\", encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            user = row[\"Author\"]\n",
    "            video_id = row[\"Video ID\"]\n",
    "            if user not in user_and_video:\n",
    "                user_and_video[user] = set()\n",
    "            user_and_video[user].add(video_id)\n",
    "user_and_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365621e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = set()\n",
    "for week_start, videos_list in top_commented_videos.items():\n",
    "    for video in videos_list:\n",
    "        videos.add(video[0])\n",
    "print(f\"Total number of videos: {len(videos)}\")\n",
    "videos, len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09973864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_comments_files_in_one_file(channels):\n",
    "    all_comments = []\n",
    "    for channel in channels:\n",
    "        with open(f\"comments_{channel}.csv\", \"r\", encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                row[\"Channel\"] = channel\n",
    "                all_comments.append(row)\n",
    "    with open(\"all_comments.csv\", \"w\", newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=all_comments[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_comments)\n",
    "collapse_comments_files_in_one_file(channels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
