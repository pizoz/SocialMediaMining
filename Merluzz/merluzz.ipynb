{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = open(\"../esercizi_classe/api_key.txt\").read()\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDfromName(name):\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        q=name,\n",
    "        type=\"channel\",\n",
    "        maxResults=5\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items'][0]['id']['channelId']\n",
    "name = \"TheMerluzz\"\n",
    "channel_id = getIDfromName(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UUBzFCho7e5KLWlfoV-V3p7Q'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getChannelPlaylist(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "playlist_id = getChannelPlaylist(channel_id)\n",
    "playlist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "def getLast100Videos(playlist_id):\n",
    "    video_ids = []\n",
    "    nextPageToken = None\n",
    "    while len(video_ids) < 100:\n",
    "        if nextPageToken:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50\n",
    "            )\n",
    "        response = request.execute()\n",
    "        video_ids += [item['snippet']['resourceId']['videoId'] for item in response['items']]\n",
    "        nextPageToken = response.get('nextPageToken')\n",
    "    return video_ids\n",
    "video_ids = getLast100Videos(playlist_id)\n",
    "print(len(video_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FtR5ItvfwPo 6\n",
      "ctfbt7Teew4 253\n",
      "T_W2UDjV7jA 33\n",
      "ZN0FBpfJSDI 31\n",
      "hiAeVH32xFg 174\n",
      "hpA3WWZAbC8 19\n",
      "Zdna0k812M8 105\n",
      "EEquT7LzTRQ 42\n",
      "HsNooSGlfFs 313\n",
      "BY9ppf2M3kE 17\n",
      "ptKHfphOnA8 102\n",
      "QNR5zQstD9k 23\n",
      "NF9JqlB5fCk 128\n",
      "EFxzpD1iq6I 35\n",
      "CVTnit4u53I 53\n",
      "Ai9AtLzGLqg 18\n",
      "cxXXEf3G_Gc 11\n",
      "ekeVWvV-hyo 284\n",
      "p1u0PyBA_gg 80\n",
      "DaEuAIrbG5U 152\n",
      "Xx-OJ_I2DSI 49\n",
      "ncyO4QyFGXU 101\n",
      "O2czpHCkARI 21\n",
      "HJtktoJOY1Q 123\n",
      "XYZG5VAxtFI 311\n",
      "h1EQDPpUFxk 239\n",
      "vvEFWCyGs4Y 263\n",
      "gfgLy48PwJM 48\n",
      "bkU_FMrITWY 85\n",
      "s552weZLwgg 11\n",
      "8IuTnVroU3M 171\n",
      "NyFy5hmSKak 154\n",
      "hA6eZObs-VU 57\n",
      "GLCBJehRyr4 36\n",
      "YxJ-bnTTuL0 3\n",
      "CZnSVTpfLpc 95\n",
      "Rl4onTr3IAc 136\n",
      "wxSoIP5amzw 55\n",
      "4IwEDaG28uA 15\n",
      "dWW9IvtBOqc 296\n",
      "RjMTlVMgEUg 9\n",
      "buMfenWth7I 99\n",
      "ey6L3O7VWYg 28\n",
      "WhUwJKCAsxk 53\n",
      "fEvaSYbWQaU 55\n",
      "GOXbwHPxgvs 421\n",
      "ikgG2QvjnjQ 11\n",
      "LXvrw-HkutI 72\n",
      "Q-oTpChMjPE 26\n",
      "FSGzN-wgo6Q 48\n",
      "V1alugwhkR8 105\n",
      "152AXSdIQxM 63\n",
      "phIi_8FeUT0 31\n",
      "zd-O24m_60o 133\n",
      "_R8f_GIZFTI 30\n",
      "FNk7rtqdxMU 213\n",
      "RUJD3aAMATM 8\n",
      "h3t0vBgDyLY 197\n",
      "KG6Xk6sKTGU 61\n",
      "G1yyfGEmd7w 73\n",
      "UzP27aY653Y 5\n",
      "wrgc2BrRVF0 143\n",
      "6iV6XuWWw7o 12\n",
      "yhwnFNb9uys 102\n",
      "7NsB4IXDrFQ 21\n",
      "_K0OJmusDLo 165\n",
      "h_SPWSMaGec 34\n",
      "Kpw3C3LUwdw 59\n",
      "wa3b9LBQkOQ 10\n",
      "OjsnqAuKEmw 144\n",
      "VBQNtUGPvzE 315\n",
      "58VqRBy9rDg 43\n",
      "xgSg08kaWPQ 38\n",
      "BhQRb2uFbd0 106\n",
      "BdyFlMN3fMU 51\n",
      "7ySoAZMXXB0 23\n",
      "YZ_4NmXWU-I 148\n",
      "mZALCV1fmKo 106\n",
      "1FS-hO6a-Og 56\n",
      "Q2eHclv25Ys 53\n",
      "_NkKXQDNLU4 10\n",
      "5CrL6xILeLI 49\n",
      "rSmysFCItUk 26\n",
      "NX442oPsuS8 97\n",
      "MXQ0hznNk_w 54\n",
      "nayt6RxKRB0 230\n",
      "zeLYc71VLDM 130\n",
      "OAvGIB4iE1g 186\n",
      "w5RqbFYbcvY 25\n",
      "z5YJ5Bz3qrI 164\n",
      "UXPLQCyYP3Q 54\n",
      "s9-bazx2rwM 158\n",
      "zg-8YVt80U4 51\n",
      "bbHwN7Xq1Tw 72\n",
      "RSV-5X-UVNc 321\n",
      "iiUB1733L-Y 39\n",
      "kL_qefTKZO8 216\n",
      "NNg_Qs1xgic 696\n",
      "Y8vQmYkJLq0 84\n",
      "17O7NMi8cZM 132\n",
      "10312\n"
     ]
    }
   ],
   "source": [
    "def getCommentatorsForVideo(video_id,number):\n",
    "    commentators, nextPageToken = OnePageCommentators(video_id,number)\n",
    "    \n",
    "    while nextPageToken:\n",
    "        newComments = []\n",
    "        newComments, nextPageToken = OnePageCommentators(video_id, nextPageToken=nextPageToken, number=number)\n",
    "        commentators.extend(newComments)\n",
    "    return commentators\n",
    "\n",
    "def OnePageCommentators(video_id, number, nextPageToken=None):\n",
    "    commentators = []\n",
    "    if nextPageToken:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=number,\n",
    "            pageToken=nextPageToken\n",
    "        )\n",
    "    else:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=number\n",
    "        )\n",
    "    \n",
    "    response = request.execute()\n",
    "    # print(\"Response items: \", len(response['items']))\n",
    "    for item in response['items']:\n",
    "        commentators.append(item['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
    "        # if 'replies' in item:\n",
    "        #     # print(\"Replies\", len(item['replies']['comments']))\n",
    "        #     for reply in item['replies']['comments']:\n",
    "        #         commentators.append(reply['snippet']['authorDisplayName'])\n",
    "    \n",
    "    nextPageToken = response.get('nextPageToken', None)\n",
    "    \n",
    "    return commentators, nextPageToken\n",
    "\n",
    "def getCommentatorsForAllVideos(video_ids,number):\n",
    "    commentators = dict()\n",
    "    sum = 0\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            commentators[video_id] = getCommentatorsForVideo(video_id,number)\n",
    "            print(video_id, len(commentators[video_id]))\n",
    "            sum += len(commentators[video_id])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    print(sum)\n",
    "    return commentators\n",
    "\n",
    "commenters = getCommentatorsForAllVideos(video_ids,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_comments(video_id):\n",
    "    commenters_words = dict()\n",
    "    nextPageToken = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "                text = item['snippet']['topLevelComment']['snippet']['textDisplay'].split()\n",
    "                text = [word.lower() for word in text]\n",
    "                if author in commenters_words:\n",
    "                    commenters_words[author].extend(text)\n",
    "                else:\n",
    "                    commenters_words[author] = text\n",
    "            \n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "            if not nextPageToken:\n",
    "                break  # No more pages, exit loop\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return commenters_words\n",
    "\n",
    "\n",
    "def get_words_from_all_videos(video_ids):\n",
    "    commenters_words = dict()\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            words_dict = get_words_from_comments(video_id)\n",
    "            for author, words in words_dict.items():\n",
    "                if author in commenters_words:\n",
    "                    commenters_words[author].update(words)\n",
    "                else:\n",
    "                    commenters_words[author] = set(words)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return commenters_words\n",
    "\n",
    "\n",
    "commenters_words = get_words_from_all_videos(video_ids)\n",
    "print(commenters_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_first_interaction_graph(commenters_words):\n",
    "    G = nx.Graph()\n",
    "    for author1, author2 in itertools.combinations(commenters_words.keys(), 2):\n",
    "        words1 = commenters_words[author1]\n",
    "        words2 = commenters_words[author2]\n",
    "        weight = get_weight(words1, words2)\n",
    "        if weight > 0:\n",
    "            G.add_edge(author1, author2, weight=weight)\n",
    "    return G\n",
    "\n",
    "def get_weight(words1, words2):\n",
    "    return abs(len(words1.intersection(words2))/len(words1.union(words2)))\n",
    "\n",
    "Grafo = make_first_interaction_graph(commenters_words)\n",
    "\n",
    "nx.write_gexf(Grafo, \"grafo.gexf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Grafo.edges()), len(Grafo.nodes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FtR5ItvfwPo\n",
      "ctfbt7Teew4\n",
      "T_W2UDjV7jA\n",
      "ZN0FBpfJSDI\n",
      "hiAeVH32xFg\n",
      "hpA3WWZAbC8\n",
      "Zdna0k812M8\n",
      "EEquT7LzTRQ\n",
      "HsNooSGlfFs\n",
      "BY9ppf2M3kE\n",
      "ptKHfphOnA8\n",
      "QNR5zQstD9k\n",
      "NF9JqlB5fCk\n",
      "EFxzpD1iq6I\n",
      "CVTnit4u53I\n",
      "Ai9AtLzGLqg\n",
      "cxXXEf3G_Gc\n",
      "ekeVWvV-hyo\n",
      "p1u0PyBA_gg\n",
      "DaEuAIrbG5U\n",
      "Xx-OJ_I2DSI\n",
      "ncyO4QyFGXU\n",
      "O2czpHCkARI\n",
      "HJtktoJOY1Q\n",
      "XYZG5VAxtFI\n",
      "h1EQDPpUFxk\n",
      "vvEFWCyGs4Y\n",
      "gfgLy48PwJM\n",
      "bkU_FMrITWY\n",
      "s552weZLwgg\n",
      "8IuTnVroU3M\n",
      "NyFy5hmSKak\n",
      "hA6eZObs-VU\n",
      "GLCBJehRyr4\n",
      "YxJ-bnTTuL0\n",
      "CZnSVTpfLpc\n",
      "Rl4onTr3IAc\n",
      "wxSoIP5amzw\n",
      "4IwEDaG28uA\n",
      "dWW9IvtBOqc\n",
      "RjMTlVMgEUg\n",
      "buMfenWth7I\n",
      "ey6L3O7VWYg\n",
      "WhUwJKCAsxk\n",
      "fEvaSYbWQaU\n",
      "GOXbwHPxgvs\n",
      "ikgG2QvjnjQ\n",
      "LXvrw-HkutI\n",
      "Q-oTpChMjPE\n",
      "FSGzN-wgo6Q\n",
      "V1alugwhkR8\n",
      "152AXSdIQxM\n",
      "phIi_8FeUT0\n",
      "zd-O24m_60o\n",
      "_R8f_GIZFTI\n",
      "FNk7rtqdxMU\n",
      "RUJD3aAMATM\n",
      "h3t0vBgDyLY\n",
      "KG6Xk6sKTGU\n",
      "G1yyfGEmd7w\n",
      "UzP27aY653Y\n",
      "wrgc2BrRVF0\n",
      "6iV6XuWWw7o\n",
      "yhwnFNb9uys\n",
      "7NsB4IXDrFQ\n",
      "_K0OJmusDLo\n",
      "h_SPWSMaGec\n",
      "Kpw3C3LUwdw\n",
      "wa3b9LBQkOQ\n",
      "OjsnqAuKEmw\n",
      "VBQNtUGPvzE\n",
      "58VqRBy9rDg\n",
      "xgSg08kaWPQ\n",
      "BhQRb2uFbd0\n",
      "BdyFlMN3fMU\n",
      "7ySoAZMXXB0\n",
      "YZ_4NmXWU-I\n",
      "mZALCV1fmKo\n",
      "1FS-hO6a-Og\n",
      "Q2eHclv25Ys\n",
      "_NkKXQDNLU4\n",
      "5CrL6xILeLI\n",
      "rSmysFCItUk\n",
      "NX442oPsuS8\n",
      "MXQ0hznNk_w\n",
      "nayt6RxKRB0\n",
      "zeLYc71VLDM\n",
      "OAvGIB4iE1g\n",
      "w5RqbFYbcvY\n",
      "z5YJ5Bz3qrI\n",
      "UXPLQCyYP3Q\n",
      "s9-bazx2rwM\n",
      "zg-8YVt80U4\n",
      "bbHwN7Xq1Tw\n",
      "RSV-5X-UVNc\n",
      "iiUB1733L-Y\n",
      "kL_qefTKZO8\n",
      "NNg_Qs1xgic\n",
      "Y8vQmYkJLq0\n",
      "17O7NMi8cZM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_videos_and_titles(playlist_id):\n",
    "    titles = dict()\n",
    "    nextPageToken = None\n",
    "    while len(titles) < 100:\n",
    "        if nextPageToken:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50\n",
    "            )\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            video_id = item['snippet']['resourceId']['videoId']\n",
    "            title = item['snippet']['title']\n",
    "            titles[video_id] = title\n",
    "        nextPageToken = response.get('nextPageToken')\n",
    "    return titles\n",
    "\n",
    "def get_videos_and_words(titles):\n",
    "    videos_title_words = dict()\n",
    "    for video_id, title in titles.items():\n",
    "        words = title.split()\n",
    "        words = [word.lower() for word in words]\n",
    "        videos_title_words[video_id] = words\n",
    "    return videos_title_words\n",
    "    \n",
    "\n",
    "video_titles = get_videos_and_titles(playlist_id)\n",
    "video_words = get_videos_and_words(video_titles)\n",
    "for video_id in video_titles:\n",
    "    print(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commenters_set(commenters):\n",
    "    commenters_set = set()\n",
    "    for video_id in commenters:\n",
    "        commenters_set.update(commenters[video_id])\n",
    "    return commenters_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commenters_dict(commenters, video_titles):\n",
    "    commenters_dict = dict()\n",
    "    for video_id in commenters:\n",
    "        for commenter in commenters[video_id]:\n",
    "            if commenter in commenters_dict:\n",
    "                commenters_dict[commenter].add((video_id, video_titles[video_id]))\n",
    "            else:\n",
    "                commenters_dict[commenter] = set([(video_id, video_titles[video_id])])\n",
    "    return commenters_dict\n",
    "\n",
    "commenters_dict = get_commenters_dict(commenters, video_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7648"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(commenters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_videos_graph(commenters_dict,video_words):\n",
    "    G = nx.Graph()\n",
    "    for author1, author2 in itertools.combinations(commenters_dict.keys(), 2):\n",
    "        videos1 = commenters_dict[author1]\n",
    "        videos2 = commenters_dict[author2]\n",
    "        shared_videos = videos1.intersection(videos2)\n",
    "        if shared_videos:\n",
    "            if len(shared_videos) > 1:\n",
    "                G.add_edge(author1, author2, weight=len(shared_videos),sim=get_sim(shared_videos, video_words))\n",
    "    return G\n",
    "\n",
    "def get_sim(shared_videos, video_words):\n",
    "    sim = 0\n",
    "    for video1, video2 in itertools.combinations(shared_videos, 2):\n",
    "        words1 = video_words[video1[0]]\n",
    "        words2 = video_words[video2[0]]\n",
    "        intersection_len = len([word for word in words1 if word in words2])\n",
    "        union_len = len(words1) + len(words2) - intersection_len\n",
    "        sim += intersection_len/union_len\n",
    "    return sim\n",
    "\n",
    "video_words = get_videos_and_words(video_titles)\n",
    "Grafo2 = shared_videos_graph(commenters_dict, video_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9009, 1032)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Grafo2.edges()), len(Grafo2.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(Grafo2, \"grafo2.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for edge in Grafo2.edges():\n",
    "    # print the edge and the sim value\n",
    "    if Grafo2[edge[0]][edge[1]]['weight'] > 1:\n",
    "        # add all the edges to json file\n",
    "        with open('edges.json', 'a') as f:\n",
    "            f.write(json.dumps({'source': edge[0], 'target': edge[1], 'weight': Grafo2[edge[0]][edge[1]]['weight'], 'sim': Grafo2[edge[0]][edge[1]]['sim']}))\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
